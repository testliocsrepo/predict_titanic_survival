{
 "metadata": {
  "name": "",
  "signature": "sha256:3b3c181f6bde9c0bb01ddc8f01f0d93823c0d5f58a06c34e52bf9414d5791f18"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import sys\n",
      "import pandas as pd\n",
      "import math\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "from scipy.stats import entropy \n",
      "\n",
      "# turn of data table rendering\n",
      "pd.set_option('display.notebook_repr_html', False)\n",
      "sys.version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "'2.7.8 |Anaconda 2.1.0 (64-bit)| (default, Jul  2 2014, 15:12:11) [MSC v.1500 64 bit (AMD64)]'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example\n",
      "We want to use a toy data set with some features to predict whether a person is happy or not. We want to build an efficient decision tree to predict the label based on new data. In this example, we examine Shannon's Entropy and Information Gain concepts to efficiently separate data points based on feature nodes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('data/entropy.csv')\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "  gender personality work  happiness\n",
        "0    man        rude   no  not happy\n",
        "1    man        nice   no  not happy\n",
        "2  woman        rude   no      happy\n",
        "3    man        nice  yes      happy\n",
        "4  woman        nice  yes      happy"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Entropy\n",
      "Entropy is a measure of unpredictability of information content expressed in bits. If we build a [decision tree](http://unsupervised-learning.com/decision-tree-classifier-on-guitar-models-in-python/), for instance, we first have to choose the starting node on which to split the data. We do this by determining the information gain of each feature we have. The more information we gain by splitting the data on a certain feature the better. [Claude Shannon's Entropy Formula](http://crackingthenutshell.com/what-is-information-part-2a-information-theory/) where entropy is denoted as $H$:\n",
      "\n",
      "$$\n",
      "H(X)=-\\sum_{i=1}^n p(X_i)\\cdot log_2p(X_i)\n",
      "$$\n",
      "\n",
      "In this example, it is the negative sum over all the data points of a label $X$ (outcome) times the log base 2 of all data points of that label. Here's an example of calculating the entropy of the happiness labels from our data set, where each possible label has a probability of $0.5$:\n",
      "\n",
      "$$\n",
      "H(X)=-p(x_0)\\cdot log_2p(x_0)-p(x_1)\\cdot log_2p(x_1)\n",
      "$$\n",
      "\n",
      "$$\n",
      "H(happiness)=-p(happy)\\cdot log_2p(happy)-p(not\\ happy)\\cdot log_2p(not\\ happy)\\\\\n",
      "H(happiness)=-0.5\\cdot log_2(0.5)-0.5\\cdot log_2(0.5)=1.0\\\\\n",
      "$$\n",
      "\n",
      "In code it looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the happiness label entropy\n",
      "n = len(df)  # number of data points\n",
      "n_happy = float(df.happiness.value_counts().happy)\n",
      "p_happy = n_happy / n  # probability of the happy label in the data set\n",
      "p_nothappy = 1.0 - p_happy  # complementary label\n",
      "\n",
      "entropy_happiness = -p_happy * math.log(p_happy, 2) - p_nothappy * math.log(p_nothappy, 2)\n",
      "entropy_happiness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "0.9709505944546686"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This set of labels is impure since it contains a variety of the labels (happy and not happy). But what if we had four happy and only one not happy label?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the happiness label entropy\n",
      "p_happy = 4.0 / n  # higher probability of the happy label\n",
      "p_nothappy = 1.0 - p_happy  # lower value for the complementary label\n",
      "\n",
      "entropy_happiness_2 = -p_happy * math.log(p_happy, 2) - p_nothappy * math.log(p_nothappy, 2)\n",
      "entropy_happiness_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "0.7219280948873623"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The label happiness has now a lower entropy. The data is stil not completely pure, but better than before. So what if we had only happy labels?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the happiness label entropy\n",
      "p_happy = 5.0 / n  # all labels are happy now\n",
      "p_nothappy = 1.0 - p_happy  # complementary label is zero now\n",
      "\n",
      "entropy_happiness_3 = -p_happy * math.log(p_happy, 2) - 0\n",
      "entropy_happiness_3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.0"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We get an entropy of zero, because the data points for happiness all have the same value; happy. This is the purest state of the data. In other words, it has the least amount of uncertainty."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Another, more convenient, way to calculate entropy is by using \n",
      "# the entropy function from the scipy.stats library\n",
      "p_happy = .75\n",
      "p_nothappy = .25\n",
      "\n",
      "entropy([p_happy, p_nothappy], base=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.81127812445913283"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Information Gain\n",
      "We use Shannon's concept of information Gain to decide which feature to use to split the data in our decision tree. The goal is the choose the feature that maximizes the information gain. The information gain is calculated with the following formula:\n",
      "\n",
      "$$\n",
      "information\\ gain=entropy(parent)- \\begin{bmatrix}weighted\\\\average\\end{bmatrix}entropy(children)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First calculate the entropy of happiness if we split\n",
      "# the data based on gender=man\n",
      "n_man = 3.0  # three labels left after splitting\n",
      "p_man_happy = 1.0 / n_man  # one happy label\n",
      "p_man_nothappy = 2.0 / n_man  # two not happy labels\n",
      "\n",
      "entropy_man_happiness = entropy([p_man_happy, p_man_nothappy], base=2)\n",
      "entropy_man_happiness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.91829583405448945"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then calculate the entropy of happiness if we split\n",
      "# the data based on gender=woman\n",
      "n_woman = 2.0  # two labels left after splitting\n",
      "p_woman_happy = 1.0 / n_woman  # one happy label\n",
      "p_woman_nothappy = 1.0 / n_woman # one not happy label\n",
      "\n",
      "entropy_woman_happiness = entropy([p_woman_happy, p_woman_nothappy], base=2)\n",
      "entropy_woman_happiness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the weighted average of the entropy of the children\n",
      "happiness_split_by_man = 3.0 / 5.0\n",
      "happiness_split_by_woman = 1 - happiness_split_by_man\n",
      "\n",
      "weighted_avg_entropy_children = \\\n",
      "    (happiness_split_by_man * entropy_man_happiness - \\\n",
      "     happiness_split_by_woman * entropy_woman_happiness)\n",
      "weighted_avg_entropy_children"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.15097750043269365"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the information gain if we split based on gender\n",
      "entropy_happiness = 0.9709505944546686  # hard coded entropy of the parent\n",
      "information_gain_split_gender = entropy_happiness - weighted_avg_entropy_children\n",
      "information_gain_split_gender"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.81997309402197494"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we build a decision tree, the information gain value is relative to the other possible ways to split the data. Therefore, we need to calculate the other features as well. A quick calculation gives us the information gain values if we split the complete data set based on the following features:\n",
      "\n",
      "|Feature|Information Gain|\n",
      "|:---|:---|\n",
      "|gender|0.81997309402197494|\n",
      "|personality|0.9709505944546686|\n",
      "|work|0.9709505944546686|\n",
      "\n",
      "With this result, we can either split on personality or work since they have both the largest information gain."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}